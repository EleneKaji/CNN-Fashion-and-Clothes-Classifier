{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split # loads data either in chunks or full\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets # open datasets\n",
    "from torchvision.transforms import ToTensor # transfor data to tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root='./data/', train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.MNIST(root='./data/', train=False, download=True, transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_size = int(len(train_data) * 0.8)  # 80% of the data for training\n",
    "val_size = len(train_data) - train_size  # 20% of the data for validation\n",
    "train_subset, val_subset = random_split(train_data, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, batch_size=batch_size) \n",
    "val_dataloader = DataLoader(val_subset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 6, Number: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "idx = torch.randint(0, 64, (1,)).item()\n",
    "image = images[idx].squeeze()\n",
    "label = labels[idx]\n",
    "print(f\"Name: {label.item()}, Number: {label.item()}\")\n",
    "plt.imshow(image, cmap=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, parameters, lr, name):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.name = name\n",
    "\n",
    "        self.momentum = 0.9\n",
    "        self.velocities = [torch.zeros_like(p) for p in parameters]\n",
    "\n",
    "        self.decay_rate = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.sq_grads = [torch.zeros_like(p) for p in parameters]\n",
    "\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.m_t = [torch.zeros_like(p) for p in parameters]\n",
    "        self.v_t = [torch.zeros_like(p) for p in parameters]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        if self.name == \"stochastic\":\n",
    "            self.stochastic_step()\n",
    "        elif self.name == \"momentum\":\n",
    "            self.momentum_step()\n",
    "        elif self.name == \"RMSprop\":\n",
    "            self.RMSprop_step()\n",
    "        elif self.name == \"Adam\":\n",
    "            self.Adam_step()\n",
    "        else:\n",
    "            print(\"no valid optimizer with such name\")\n",
    "\n",
    "    def stochastic_step(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters:\n",
    "                if param is not None:\n",
    "                    param -= param.grad * self.lr\n",
    "                    # param.grad.zero_()\n",
    "\n",
    "    def momentum_step(self):\n",
    "        with torch.no_grad():\n",
    "            for param, velocity in zip(self.parameters, self.velocities):\n",
    "                if param is not None:\n",
    "                    # view momentum as the mass that scales the last velocity a bit but still gives more direction\n",
    "                    # the rest is just the stochatic gradient descent\n",
    "                    velocity.mul_(self.momentum).add_(param.grad, alpha=self.lr)\n",
    "                    param.sub_(velocity)\n",
    "                    # param.grad.zero_()\n",
    "\n",
    "                    # another way\n",
    "                    # higher the momentum closer it pays attention to the upcoming points\n",
    "                    # lower the momentum closer it pays attention to the previous points\n",
    "                    # velocity.mul_(self.momentum).add_(param.grad, alpha=(1 - self.momentum))\n",
    "                    # param.sub_(velocity, alpha=self.lr)\n",
    "                    # param.grad.zero_()\n",
    "    \n",
    "    def RMSprop_step(self):\n",
    "        with torch.no_grad():\n",
    "            for param, sq_grad in zip(self.parameters, self.sq_grads):\n",
    "                if param.grad is not None:\n",
    "                    sq_grad.mul_(self.decay_rate).addcmul_(param.grad, param.grad, value=1 - self.decay_rate)\n",
    "                    sqrt = sq_grad.sqrt().add(self.epsilon)\n",
    "                    adjusted_grad = param.grad / sqrt\n",
    "                    param.sub_(adjusted_grad, alpha=self.lr)\n",
    "\n",
    "    def Adam_step(self):\n",
    "        with torch.no_grad():\n",
    "            self.t += 1\n",
    "            for param, m, v in zip(self.parameters, self.m_t, self.v_t):\n",
    "                if param is not None:\n",
    "                    m.mul_(self.beta1).add_(param.grad, alpha=1 - self.beta1)\n",
    "                    v.mul_(self.beta2).addcmul_(param.grad, param.grad, value=1 - self.beta2)\n",
    "                    m_hat = m / (1 - self.beta1 ** self.t)\n",
    "                    v_hat = v / (1 - self.beta2 ** self.t)\n",
    "                    param.sub_(m_hat / (v_hat.sqrt().add(self.epsilon)), alpha=self.lr)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    def __init__(self, out_channels, in_channels, kernel_size, stride=1, padding=0, processor=\"cuda:0\"):\n",
    "        self.w = torch.randn((out_channels, in_channels, kernel_size[0], kernel_size[1]), device=processor, requires_grad=True)\n",
    "        self.b = torch.randn(out_channels, device=processor, requires_grad=True)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        out_channels, _, kernel_height, kernel_width = self.w.shape\n",
    "\n",
    "        out_height = (in_height - kernel_height + 2 * self.padding) // self.stride + 1\n",
    "        out_width = (in_width - kernel_width + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        unfolded_input = torch.zeros(batch_size, out_height, out_width, in_channels * kernel_height * kernel_width, device=self.processor) # [64, 27, 27, 4]\n",
    "\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                start_i = i * self.stride\n",
    "                end_i = start_i + kernel_height\n",
    "                start_j = j * self.stride\n",
    "                end_j = start_j + kernel_width\n",
    "\n",
    "                patch = input[:, :, start_i:end_i, start_j:end_j]\n",
    "                patch = patch.reshape(batch_size, -1)\n",
    "                \n",
    "                unfolded_input[:, i, j, :] = patch\n",
    "        \n",
    "        unfolded_input = unfolded_input.view(batch_size * out_height * out_width, in_channels * kernel_height * kernel_width)\n",
    "        unfolded_kernel = self.w.view(out_channels, -1).t()\n",
    "        \n",
    "        unfolded_output = torch.matmul(unfolded_input, unfolded_kernel)  # [64 * 27 * 27, 12]\n",
    "        output = unfolded_output.view(batch_size, out_height, out_width, out_channels)  # [64, 27, 27, 12]\n",
    "        output = output.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        output += self.b.view(1, out_channels, 1, 1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d:\n",
    "    def __init__(self, w_size, processor=\"cuda:0\"):\n",
    "        self.w_size = w_size\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, input):\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        kernel_height, kernel_width = self.w_size\n",
    "\n",
    "        out_height = in_height // kernel_height\n",
    "        out_width = in_width // kernel_width\n",
    "\n",
    "        output = torch.zeros(batch_size, in_channels, out_height, out_width, device=self.processor) # [64, 27, 27, 4]\n",
    "\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                start_i = i * kernel_height\n",
    "                end_i = start_i + kernel_height\n",
    "                start_j = j * kernel_width\n",
    "                end_j = start_j + kernel_width\n",
    "\n",
    "                window = input[:, :, start_i:end_i, start_j:end_j]\n",
    "                max_values_height, _ = window.max(dim=2, keepdim=True)  # Compute max along height dimension\n",
    "                max_values, _ = max_values_height.max(dim=3, keepdim=True)  # Compute max along width dimension\n",
    "                output[:, :, i, j] = max_values.squeeze() # do not first dim as 1\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer: \n",
    "    # nin is the number of input (prev layer)\n",
    "    # nout is the number of output (next layer)\n",
    "    def __init__(self, nin, nout, processor):\n",
    "        self.w = torch.randn((nin, nout), requires_grad=True, device=processor)\n",
    "        self.b = torch.randn(1, requires_grad=True, device=processor)\n",
    "    \n",
    "    # x is the input in (batch, input)\n",
    "    def __call__(self, x):\n",
    "        eq = torch.matmul(x, self.w) + self.b\n",
    "        out = torch.tanh(eq)\n",
    "        return out # returns (batch, output # of neurons for next layer)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts, processor=\"cuda:0\"):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1], processor) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN():\n",
    "    def __init__(self, *layers, lr, optimizer=\"stochastic\"):\n",
    "        self.layers = layers\n",
    "        self.optimizer = Optimizer(self.parameters(), lr, optimizer)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grads(self):\n",
    "        self.optimizer.zero_grads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cnn, loss_fn):\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(\"cuda:0\"), y.to(\"cuda:0\")\n",
    "        \n",
    "        output = cnn(x)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        cnn.zero_grads()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        cnn.update_parameters()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Loss: {loss:>7f}\\t [{((batch + 1) * 64):>5d}/{937 * 64}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(cnn, loss_fn):\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dataloader:\n",
    "            x, y = x.to(\"cuda:0\"), y.to(\"cuda:0\")\n",
    "\n",
    "            output = cnn(x)\n",
    "\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "            tot += y.size(0)\n",
    "\n",
    "    accuracy = correct / tot * 100\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(cnn):\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_x, test_y in test_dataloader:\n",
    "            test_x, test_y = test_x.to(\"cuda:0\"), test_y.to(\"cuda:0\")\n",
    "\n",
    "            output = cnn(test_x)\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(test_y.view_as(pred)).sum().item()\n",
    "            tot += test_y.size(0)\n",
    "\n",
    "    accuracy = correct / tot * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(Conv2d(4, 1, (1, 1)), \n",
    "          MaxPool2d((2, 2)), \n",
    "          MLP(784, [64, 32, 10]), lr=0.001, optimizer=\"Adam\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-----------------------------------------\n",
      "Loss: 1.972763\t [   64/59968]\n",
      "Loss: 1.991168\t [ 6464/59968]\n",
      "Loss: 1.989024\t [12864/59968]\n",
      "Loss: 1.994237\t [19264/59968]\n",
      "Loss: 1.995692\t [25664/59968]\n",
      "Loss: 1.990796\t [32064/59968]\n",
      "Loss: 1.997360\t [38464/59968]\n",
      "Loss: 1.982796\t [44864/59968]\n",
      "Loss: 2.000590 Accuracy: 90.891667\n",
      "\n",
      "Epoch 2\n",
      "-----------------------------------------\n",
      "Loss: 1.973845\t [   64/59968]\n",
      "Loss: 1.993311\t [ 6464/59968]\n",
      "Loss: 1.982967\t [12864/59968]\n",
      "Loss: 1.995503\t [19264/59968]\n",
      "Loss: 1.986946\t [25664/59968]\n",
      "Loss: 1.985104\t [32064/59968]\n",
      "Loss: 1.996280\t [38464/59968]\n",
      "Loss: 1.984999\t [44864/59968]\n",
      "Loss: 1.986740 Accuracy: 91.066667\n",
      "\n",
      "Epoch 3\n",
      "-----------------------------------------\n",
      "Loss: 1.972969\t [   64/59968]\n",
      "Loss: 1.993455\t [ 6464/59968]\n",
      "Loss: 1.979751\t [12864/59968]\n",
      "Loss: 1.999653\t [19264/59968]\n",
      "Loss: 1.984933\t [25664/59968]\n",
      "Loss: 1.986074\t [32064/59968]\n",
      "Loss: 2.005311\t [38464/59968]\n",
      "Loss: 1.979626\t [44864/59968]\n",
      "Loss: 1.995536 Accuracy: 90.991667\n",
      "\n",
      "Epoch 4\n",
      "-----------------------------------------\n",
      "Loss: 1.976463\t [   64/59968]\n",
      "Loss: 1.994103\t [ 6464/59968]\n",
      "Loss: 2.001505\t [12864/59968]\n",
      "Loss: 2.006863\t [19264/59968]\n",
      "Loss: 1.985342\t [25664/59968]\n",
      "Loss: 1.985425\t [32064/59968]\n",
      "Loss: 2.001012\t [38464/59968]\n",
      "Loss: 1.983837\t [44864/59968]\n",
      "Loss: 2.014153 Accuracy: 90.925000\n",
      "\n",
      "Epoch 5\n",
      "-----------------------------------------\n",
      "Loss: 1.972027\t [   64/59968]\n",
      "Loss: 1.995959\t [ 6464/59968]\n",
      "Loss: 1.989410\t [12864/59968]\n",
      "Loss: 1.995685\t [19264/59968]\n",
      "Loss: 1.985464\t [25664/59968]\n",
      "Loss: 1.985254\t [32064/59968]\n",
      "Loss: 2.002929\t [38464/59968]\n",
      "Loss: 1.981507\t [44864/59968]\n",
      "Loss: 2.031458 Accuracy: 91.116667\n",
      "\n",
      "Epoch 6\n",
      "-----------------------------------------\n",
      "Loss: 1.969736\t [   64/59968]\n",
      "Loss: 1.992946\t [ 6464/59968]\n",
      "Loss: 1.991439\t [12864/59968]\n",
      "Loss: 1.997965\t [19264/59968]\n",
      "Loss: 1.987138\t [25664/59968]\n",
      "Loss: 1.986970\t [32064/59968]\n",
      "Loss: 2.000886\t [38464/59968]\n",
      "Loss: 1.977788\t [44864/59968]\n",
      "Loss: 2.010613 Accuracy: 91.316667\n",
      "\n",
      "Epoch 7\n",
      "-----------------------------------------\n",
      "Loss: 1.973099\t [   64/59968]\n",
      "Loss: 1.991544\t [ 6464/59968]\n",
      "Loss: 1.986738\t [12864/59968]\n",
      "Loss: 1.996090\t [19264/59968]\n",
      "Loss: 1.987255\t [25664/59968]\n",
      "Loss: 1.985608\t [32064/59968]\n",
      "Loss: 2.001758\t [38464/59968]\n",
      "Loss: 1.977983\t [44864/59968]\n",
      "Loss: 2.004390 Accuracy: 91.258333\n",
      "\n",
      "Epoch 8\n",
      "-----------------------------------------\n",
      "Loss: 1.971198\t [   64/59968]\n",
      "Loss: 1.988629\t [ 6464/59968]\n",
      "Loss: 1.990943\t [12864/59968]\n",
      "Loss: 1.994420\t [19264/59968]\n",
      "Loss: 1.985626\t [25664/59968]\n",
      "Loss: 1.984843\t [32064/59968]\n",
      "Loss: 1.999936\t [38464/59968]\n",
      "Loss: 1.975764\t [44864/59968]\n",
      "Loss: 2.026166 Accuracy: 91.391667\n",
      "\n",
      "Epoch 9\n",
      "-----------------------------------------\n",
      "Loss: 1.974950\t [   64/59968]\n",
      "Loss: 1.990093\t [ 6464/59968]\n",
      "Loss: 1.987973\t [12864/59968]\n",
      "Loss: 1.994064\t [19264/59968]\n",
      "Loss: 1.984816\t [25664/59968]\n",
      "Loss: 1.986547\t [32064/59968]\n",
      "Loss: 2.000973\t [38464/59968]\n",
      "Loss: 1.977818\t [44864/59968]\n",
      "Loss: 1.990227 Accuracy: 91.200000\n",
      "\n",
      "Epoch 10\n",
      "-----------------------------------------\n",
      "Loss: 1.972434\t [   64/59968]\n",
      "Loss: 1.989210\t [ 6464/59968]\n",
      "Loss: 1.984147\t [12864/59968]\n",
      "Loss: 1.994982\t [19264/59968]\n",
      "Loss: 1.984923\t [25664/59968]\n",
      "Loss: 1.986890\t [32064/59968]\n",
      "Loss: 1.999855\t [38464/59968]\n",
      "Loss: 1.978053\t [44864/59968]\n",
      "Loss: 2.010182 Accuracy: 91.558333\n",
      "\n",
      "Epoch 11\n",
      "-----------------------------------------\n",
      "Loss: 1.969533\t [   64/59968]\n",
      "Loss: 1.987506\t [ 6464/59968]\n",
      "Loss: 1.974139\t [12864/59968]\n",
      "Loss: 1.996775\t [19264/59968]\n",
      "Loss: 1.987326\t [25664/59968]\n",
      "Loss: 1.984769\t [32064/59968]\n",
      "Loss: 2.000462\t [38464/59968]\n",
      "Loss: 1.977975\t [44864/59968]\n",
      "Loss: 1.999532 Accuracy: 91.325000\n",
      "\n",
      "Epoch 12\n",
      "-----------------------------------------\n",
      "Loss: 1.969124\t [   64/59968]\n",
      "Loss: 1.986104\t [ 6464/59968]\n",
      "Loss: 1.987407\t [12864/59968]\n",
      "Loss: 1.995570\t [19264/59968]\n",
      "Loss: 1.986956\t [25664/59968]\n",
      "Loss: 1.985512\t [32064/59968]\n",
      "Loss: 1.999994\t [38464/59968]\n",
      "Loss: 1.978070\t [44864/59968]\n",
      "Loss: 2.011402 Accuracy: 91.408333\n",
      "\n",
      "Epoch 13\n",
      "-----------------------------------------\n",
      "Loss: 1.972830\t [   64/59968]\n",
      "Loss: 1.982563\t [ 6464/59968]\n",
      "Loss: 1.978820\t [12864/59968]\n",
      "Loss: 1.995230\t [19264/59968]\n",
      "Loss: 1.986834\t [25664/59968]\n",
      "Loss: 1.984360\t [32064/59968]\n",
      "Loss: 1.995672\t [38464/59968]\n",
      "Loss: 1.977654\t [44864/59968]\n",
      "Loss: 2.001706 Accuracy: 91.491667\n",
      "\n",
      "Epoch 14\n",
      "-----------------------------------------\n",
      "Loss: 1.969940\t [   64/59968]\n",
      "Loss: 1.986469\t [ 6464/59968]\n",
      "Loss: 1.974387\t [12864/59968]\n",
      "Loss: 1.994962\t [19264/59968]\n",
      "Loss: 1.986488\t [25664/59968]\n",
      "Loss: 1.985925\t [32064/59968]\n",
      "Loss: 1.993500\t [38464/59968]\n",
      "Loss: 1.976863\t [44864/59968]\n",
      "Loss: 2.007514 Accuracy: 91.541667\n",
      "\n",
      "Epoch 15\n",
      "-----------------------------------------\n",
      "Loss: 1.973668\t [   64/59968]\n",
      "Loss: 1.988051\t [ 6464/59968]\n",
      "Loss: 1.975657\t [12864/59968]\n",
      "Loss: 1.995683\t [19264/59968]\n",
      "Loss: 1.987142\t [25664/59968]\n",
      "Loss: 1.984530\t [32064/59968]\n",
      "Loss: 1.992631\t [38464/59968]\n",
      "Loss: 1.973377\t [44864/59968]\n",
      "Loss: 2.013211 Accuracy: 91.683333\n",
      "\n",
      "Epoch 16\n",
      "-----------------------------------------\n",
      "Loss: 1.969137\t [   64/59968]\n",
      "Loss: 1.987154\t [ 6464/59968]\n",
      "Loss: 1.973686\t [12864/59968]\n",
      "Loss: 1.995039\t [19264/59968]\n",
      "Loss: 1.985094\t [25664/59968]\n",
      "Loss: 1.984171\t [32064/59968]\n",
      "Loss: 1.989341\t [38464/59968]\n",
      "Loss: 1.973842\t [44864/59968]\n",
      "Loss: 2.007185 Accuracy: 91.466667\n",
      "\n",
      "Epoch 17\n",
      "-----------------------------------------\n",
      "Loss: 1.970159\t [   64/59968]\n",
      "Loss: 1.989373\t [ 6464/59968]\n",
      "Loss: 1.974434\t [12864/59968]\n",
      "Loss: 1.995227\t [19264/59968]\n",
      "Loss: 1.987928\t [25664/59968]\n",
      "Loss: 1.985269\t [32064/59968]\n",
      "Loss: 1.986674\t [38464/59968]\n",
      "Loss: 1.977994\t [44864/59968]\n",
      "Loss: 2.000413 Accuracy: 91.600000\n",
      "\n",
      "Epoch 18\n",
      "-----------------------------------------\n",
      "Loss: 1.970333\t [   64/59968]\n",
      "Loss: 1.983822\t [ 6464/59968]\n",
      "Loss: 1.974243\t [12864/59968]\n",
      "Loss: 1.994943\t [19264/59968]\n",
      "Loss: 1.985343\t [25664/59968]\n",
      "Loss: 1.984502\t [32064/59968]\n",
      "Loss: 1.990608\t [38464/59968]\n",
      "Loss: 1.975565\t [44864/59968]\n",
      "Loss: 2.005244 Accuracy: 91.533333\n",
      "\n",
      "Epoch 19\n",
      "-----------------------------------------\n",
      "Loss: 1.971141\t [   64/59968]\n",
      "Loss: 1.989987\t [ 6464/59968]\n",
      "Loss: 1.973626\t [12864/59968]\n",
      "Loss: 1.994651\t [19264/59968]\n",
      "Loss: 1.983898\t [25664/59968]\n",
      "Loss: 1.987825\t [32064/59968]\n",
      "Loss: 1.988183\t [38464/59968]\n",
      "Loss: 1.972768\t [44864/59968]\n",
      "Loss: 2.001748 Accuracy: 91.708333\n",
      "\n",
      "Epoch 20\n",
      "-----------------------------------------\n",
      "Loss: 1.968064\t [   64/59968]\n",
      "Loss: 1.987617\t [ 6464/59968]\n",
      "Loss: 1.974230\t [12864/59968]\n",
      "Loss: 1.994091\t [19264/59968]\n",
      "Loss: 1.986819\t [25664/59968]\n",
      "Loss: 1.984671\t [32064/59968]\n",
      "Loss: 1.991485\t [38464/59968]\n",
      "Loss: 1.974880\t [44864/59968]\n",
      "Loss: 2.022199 Accuracy: 91.450000\n",
      "\n",
      "Epoch 21\n",
      "-----------------------------------------\n",
      "Loss: 1.967496\t [   64/59968]\n",
      "Loss: 1.987725\t [ 6464/59968]\n",
      "Loss: 1.976122\t [12864/59968]\n",
      "Loss: 1.993870\t [19264/59968]\n",
      "Loss: 1.987549\t [25664/59968]\n",
      "Loss: 1.987448\t [32064/59968]\n",
      "Loss: 1.987564\t [38464/59968]\n",
      "Loss: 1.973837\t [44864/59968]\n",
      "Loss: 2.003558 Accuracy: 91.658333\n",
      "\n",
      "Epoch 22\n",
      "-----------------------------------------\n",
      "Loss: 1.968502\t [   64/59968]\n",
      "Loss: 1.989010\t [ 6464/59968]\n",
      "Loss: 1.974576\t [12864/59968]\n",
      "Loss: 1.994237\t [19264/59968]\n",
      "Loss: 1.985619\t [25664/59968]\n",
      "Loss: 1.984590\t [32064/59968]\n",
      "Loss: 1.987083\t [38464/59968]\n",
      "Loss: 1.973642\t [44864/59968]\n",
      "Loss: 2.013033 Accuracy: 91.641667\n",
      "\n",
      "Epoch 23\n",
      "-----------------------------------------\n",
      "Loss: 1.968172\t [   64/59968]\n",
      "Loss: 1.987609\t [ 6464/59968]\n",
      "Loss: 1.975198\t [12864/59968]\n",
      "Loss: 1.995241\t [19264/59968]\n",
      "Loss: 1.983651\t [25664/59968]\n",
      "Loss: 1.988224\t [32064/59968]\n",
      "Loss: 1.987142\t [38464/59968]\n",
      "Loss: 1.972104\t [44864/59968]\n",
      "Loss: 2.011566 Accuracy: 91.825000\n",
      "\n",
      "Epoch 24\n",
      "-----------------------------------------\n",
      "Loss: 1.967844\t [   64/59968]\n",
      "Loss: 1.988533\t [ 6464/59968]\n",
      "Loss: 1.973988\t [12864/59968]\n",
      "Loss: 1.993459\t [19264/59968]\n",
      "Loss: 1.983497\t [25664/59968]\n",
      "Loss: 1.985375\t [32064/59968]\n",
      "Loss: 1.988541\t [38464/59968]\n",
      "Loss: 1.971888\t [44864/59968]\n",
      "Loss: 2.009105 Accuracy: 91.808333\n",
      "\n",
      "Epoch 25\n",
      "-----------------------------------------\n",
      "Loss: 1.968551\t [   64/59968]\n",
      "Loss: 1.990085\t [ 6464/59968]\n",
      "Loss: 1.974601\t [12864/59968]\n",
      "Loss: 1.993519\t [19264/59968]\n",
      "Loss: 1.977800\t [25664/59968]\n",
      "Loss: 1.983807\t [32064/59968]\n",
      "Loss: 1.986378\t [38464/59968]\n",
      "Loss: 1.972292\t [44864/59968]\n",
      "Loss: 2.008205 Accuracy: 91.741667\n",
      "\n",
      "Epoch 26\n",
      "-----------------------------------------\n",
      "Loss: 1.967570\t [   64/59968]\n",
      "Loss: 1.989877\t [ 6464/59968]\n",
      "Loss: 1.974264\t [12864/59968]\n",
      "Loss: 1.991816\t [19264/59968]\n",
      "Loss: 1.983345\t [25664/59968]\n",
      "Loss: 1.989957\t [32064/59968]\n",
      "Loss: 1.990039\t [38464/59968]\n",
      "Loss: 1.971871\t [44864/59968]\n",
      "Loss: 2.008569 Accuracy: 91.958333\n",
      "\n",
      "Epoch 27\n",
      "-----------------------------------------\n",
      "Loss: 1.967526\t [   64/59968]\n",
      "Loss: 1.983962\t [ 6464/59968]\n",
      "Loss: 1.979668\t [12864/59968]\n",
      "Loss: 1.993088\t [19264/59968]\n",
      "Loss: 1.983571\t [25664/59968]\n",
      "Loss: 1.983297\t [32064/59968]\n",
      "Loss: 1.988394\t [38464/59968]\n",
      "Loss: 1.969893\t [44864/59968]\n",
      "Loss: 1.996189 Accuracy: 91.891667\n",
      "\n",
      "Epoch 28\n",
      "-----------------------------------------\n",
      "Loss: 1.967221\t [   64/59968]\n",
      "Loss: 1.984836\t [ 6464/59968]\n",
      "Loss: 1.982945\t [12864/59968]\n",
      "Loss: 1.991775\t [19264/59968]\n",
      "Loss: 1.980322\t [25664/59968]\n",
      "Loss: 1.984101\t [32064/59968]\n",
      "Loss: 1.986248\t [38464/59968]\n",
      "Loss: 1.972332\t [44864/59968]\n",
      "Loss: 2.008649 Accuracy: 91.875000\n",
      "\n",
      "Epoch 29\n",
      "-----------------------------------------\n",
      "Loss: 1.967697\t [   64/59968]\n",
      "Loss: 1.983791\t [ 6464/59968]\n",
      "Loss: 1.976890\t [12864/59968]\n",
      "Loss: 1.991634\t [19264/59968]\n",
      "Loss: 1.979842\t [25664/59968]\n",
      "Loss: 1.987782\t [32064/59968]\n",
      "Loss: 1.992342\t [38464/59968]\n",
      "Loss: 1.969103\t [44864/59968]\n",
      "Loss: 2.007239 Accuracy: 91.833333\n",
      "\n",
      "Epoch 30\n",
      "-----------------------------------------\n",
      "Loss: 1.967285\t [   64/59968]\n",
      "Loss: 1.984176\t [ 6464/59968]\n",
      "Loss: 1.977529\t [12864/59968]\n",
      "Loss: 1.990697\t [19264/59968]\n",
      "Loss: 1.978832\t [25664/59968]\n",
      "Loss: 1.984063\t [32064/59968]\n",
      "Loss: 1.987094\t [38464/59968]\n",
      "Loss: 1.971950\t [44864/59968]\n",
      "Loss: 2.007831 Accuracy: 91.925000\n",
      "\n",
      "Epoch 31\n",
      "-----------------------------------------\n",
      "Loss: 1.967642\t [   64/59968]\n",
      "Loss: 1.984517\t [ 6464/59968]\n",
      "Loss: 1.977597\t [12864/59968]\n",
      "Loss: 1.989950\t [19264/59968]\n",
      "Loss: 1.980149\t [25664/59968]\n",
      "Loss: 1.984158\t [32064/59968]\n",
      "Loss: 1.985453\t [38464/59968]\n",
      "Loss: 1.971827\t [44864/59968]\n",
      "Loss: 2.017644 Accuracy: 91.958333\n",
      "\n",
      "Epoch 32\n",
      "-----------------------------------------\n",
      "Loss: 1.971912\t [   64/59968]\n",
      "Loss: 1.986209\t [ 6464/59968]\n",
      "Loss: 1.975641\t [12864/59968]\n",
      "Loss: 1.990285\t [19264/59968]\n",
      "Loss: 1.979687\t [25664/59968]\n",
      "Loss: 1.984254\t [32064/59968]\n",
      "Loss: 1.984348\t [38464/59968]\n",
      "Loss: 1.972147\t [44864/59968]\n",
      "Loss: 2.018655 Accuracy: 91.908333\n",
      "\n",
      "Epoch 33\n",
      "-----------------------------------------\n",
      "Loss: 1.972092\t [   64/59968]\n",
      "Loss: 1.985715\t [ 6464/59968]\n",
      "Loss: 1.979531\t [12864/59968]\n",
      "Loss: 1.990011\t [19264/59968]\n",
      "Loss: 1.978889\t [25664/59968]\n",
      "Loss: 1.983783\t [32064/59968]\n",
      "Loss: 1.985374\t [38464/59968]\n",
      "Loss: 1.972222\t [44864/59968]\n",
      "Loss: 2.007870 Accuracy: 92.108333\n",
      "\n",
      "Epoch 34\n",
      "-----------------------------------------\n",
      "Loss: 1.966873\t [   64/59968]\n",
      "Loss: 1.984478\t [ 6464/59968]\n",
      "Loss: 1.979078\t [12864/59968]\n",
      "Loss: 1.991590\t [19264/59968]\n",
      "Loss: 1.979263\t [25664/59968]\n",
      "Loss: 1.983526\t [32064/59968]\n",
      "Loss: 1.985713\t [38464/59968]\n",
      "Loss: 1.971985\t [44864/59968]\n",
      "Loss: 1.996520 Accuracy: 92.150000\n",
      "\n",
      "Epoch 35\n",
      "-----------------------------------------\n",
      "Loss: 1.966067\t [   64/59968]\n",
      "Loss: 1.981246\t [ 6464/59968]\n",
      "Loss: 1.976229\t [12864/59968]\n",
      "Loss: 1.990470\t [19264/59968]\n",
      "Loss: 1.977813\t [25664/59968]\n",
      "Loss: 1.981456\t [32064/59968]\n",
      "Loss: 1.986396\t [38464/59968]\n",
      "Loss: 1.972019\t [44864/59968]\n",
      "Loss: 2.014355 Accuracy: 92.183333\n",
      "\n",
      "Epoch 36\n",
      "-----------------------------------------\n",
      "Loss: 1.970635\t [   64/59968]\n",
      "Loss: 1.983956\t [ 6464/59968]\n",
      "Loss: 1.972808\t [12864/59968]\n",
      "Loss: 1.990122\t [19264/59968]\n",
      "Loss: 1.978703\t [25664/59968]\n",
      "Loss: 1.980408\t [32064/59968]\n",
      "Loss: 1.984825\t [38464/59968]\n",
      "Loss: 1.972133\t [44864/59968]\n",
      "Loss: 1.995613 Accuracy: 92.200000\n",
      "\n",
      "Epoch 37\n",
      "-----------------------------------------\n",
      "Loss: 1.966245\t [   64/59968]\n",
      "Loss: 1.983771\t [ 6464/59968]\n",
      "Loss: 1.973120\t [12864/59968]\n",
      "Loss: 1.990171\t [19264/59968]\n",
      "Loss: 1.979188\t [25664/59968]\n",
      "Loss: 1.982225\t [32064/59968]\n",
      "Loss: 1.986394\t [38464/59968]\n",
      "Loss: 1.971814\t [44864/59968]\n",
      "Loss: 1.997430 Accuracy: 92.283333\n",
      "\n",
      "Epoch 38\n",
      "-----------------------------------------\n",
      "Loss: 1.967665\t [   64/59968]\n",
      "Loss: 1.984174\t [ 6464/59968]\n",
      "Loss: 1.971509\t [12864/59968]\n",
      "Loss: 1.990001\t [19264/59968]\n",
      "Loss: 1.978525\t [25664/59968]\n",
      "Loss: 1.980034\t [32064/59968]\n",
      "Loss: 1.991637\t [38464/59968]\n",
      "Loss: 1.968095\t [44864/59968]\n",
      "Loss: 1.998173 Accuracy: 92.283333\n",
      "\n",
      "Epoch 39\n",
      "-----------------------------------------\n",
      "Loss: 1.966377\t [   64/59968]\n",
      "Loss: 1.984160\t [ 6464/59968]\n",
      "Loss: 1.971318\t [12864/59968]\n",
      "Loss: 1.990061\t [19264/59968]\n",
      "Loss: 1.978683\t [25664/59968]\n",
      "Loss: 1.978880\t [32064/59968]\n",
      "Loss: 1.991891\t [38464/59968]\n",
      "Loss: 1.966264\t [44864/59968]\n",
      "Loss: 2.007916 Accuracy: 92.208333\n",
      "\n",
      "Epoch 40\n",
      "-----------------------------------------\n",
      "Loss: 1.966002\t [   64/59968]\n",
      "Loss: 1.985681\t [ 6464/59968]\n",
      "Loss: 1.970964\t [12864/59968]\n",
      "Loss: 1.992455\t [19264/59968]\n",
      "Loss: 1.978665\t [25664/59968]\n",
      "Loss: 1.978755\t [32064/59968]\n",
      "Loss: 1.986520\t [38464/59968]\n",
      "Loss: 1.972233\t [44864/59968]\n",
      "Loss: 2.007138 Accuracy: 92.266667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    print(f\"Epoch {i + 1}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    train(cnn, loss_fn)\n",
    "    loss, accuracy = validate(cnn, loss_fn)\n",
    "    print(f\"Loss: {loss:>7f} Accuracy: {accuracy:>5f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.54%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
