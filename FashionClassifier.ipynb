{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split # loads data either in chunks or full\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets # open datasets\n",
    "from torchvision.transforms import ToTensor # transfor data to tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choosing gpu for training\n",
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download training and test data from open datasets.\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "train_size = int(len(train_data) * 0.8)  # 80% of the data for training\n",
    "val_size = len(train_data) - train_size  # 20% of the data for validation\n",
    "train_subset, val_subset = random_split(train_data, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, batch_size=64) \n",
    "val_dataloader = DataLoader(val_subset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Ankle Boot, Number: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting and understanding images and labels with first batch\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "idx = torch.randint(0, 64, (1,)).item()\n",
    "image = images[idx].squeeze()\n",
    "label = labels[idx]\n",
    "print(f\"Name: {labels_map[label.item()]}, Number: {label.item()}\")\n",
    "plt.imshow(image, cmap=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 12, 27, 27])"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv(input, w, stride=1, padding=0, processor=\"cuda:0\"):\n",
    "    batch_size, in_channels, in_height, in_width = input.shape\n",
    "    out_channels, _, kernel_height, kernel_width = w.shape\n",
    "\n",
    "    out_height = (in_height - kernel_height + 2 * padding) // stride + 1\n",
    "    out_width = (in_width - kernel_width + 2 * padding) // stride + 1\n",
    "\n",
    "    unfolded_input = torch.zeros(batch_size, out_height, out_width, in_channels * kernel_height * kernel_width, device=processor) # [64, 27, 27, 4]\n",
    "\n",
    "    for i in range(out_height):\n",
    "        for j in range(out_width):\n",
    "            start_i = i * stride\n",
    "            end_i = start_i + kernel_height\n",
    "            start_j = j * stride\n",
    "            end_j = start_j + kernel_width\n",
    "\n",
    "            patch = input[:, :, start_i:end_i, start_j:end_j]\n",
    "            patch = patch.reshape(batch_size, -1)\n",
    "            \n",
    "            unfolded_input[:, i, j, :] = patch\n",
    "    \n",
    "    unfolded_input = unfolded_input.view(batch_size * out_height * out_width, in_channels * kernel_height * kernel_width)\n",
    "    unfolded_kernel = w.view(out_channels, -1).t()\n",
    "    \n",
    "    unfolded_output = torch.matmul(unfolded_input, unfolded_kernel)# [64 * 27 * 27, 12]\n",
    "    output = unfolded_output.view(batch_size, out_height, out_width, out_channels) # [64, 27, 27, 12]\n",
    "    output = output.permute(0, 3, 1, 2).contiguous()\n",
    "    return output\n",
    "\n",
    "\n",
    "input1 = torch.randn((64, 1, 28, 28), device=\"cuda:0\", requires_grad=True)\n",
    "w1 = torch.randn((12, 1, 2, 2), device=\"cuda:0\", requires_grad=True)\n",
    "stride = 1 \n",
    "padding = 0 \n",
    "\n",
    "output1 = conv(input1, w1, stride=stride, padding=padding)\n",
    "output1.shape\n",
    "# flatten = output1.view(output1.size(0), -1)\n",
    "# flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    def __init__(self, out_channels, in_channels, kernel_size, stride=1, padding=0, processor=\"cuda:0\"):\n",
    "        self.w = torch.randn((out_channels, in_channels, kernel_size[0], kernel_size[1]), device=processor, requires_grad=True)\n",
    "        self.b = torch.randn(out_channels, device=processor, requires_grad=True)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        out_channels, _, kernel_height, kernel_width = self.w.shape\n",
    "\n",
    "        out_height = (in_height - kernel_height + 2 * self.padding) // self.stride + 1\n",
    "        out_width = (in_width - kernel_width + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        unfolded_input = torch.zeros(batch_size, out_height, out_width, in_channels * kernel_height * kernel_width, device=self.processor) # [64, 27, 27, 4]\n",
    "\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                start_i = i * self.stride\n",
    "                end_i = start_i + kernel_height\n",
    "                start_j = j * self.stride\n",
    "                end_j = start_j + kernel_width\n",
    "\n",
    "                patch = input[:, :, start_i:end_i, start_j:end_j]\n",
    "                patch = patch.reshape(batch_size, -1)\n",
    "                \n",
    "                unfolded_input[:, i, j, :] = patch\n",
    "        \n",
    "        unfolded_input = unfolded_input.view(batch_size * out_height * out_width, in_channels * kernel_height * kernel_width)\n",
    "        unfolded_kernel = self.w.view(out_channels, -1).t()\n",
    "        \n",
    "        unfolded_output = torch.matmul(unfolded_input, unfolded_kernel)  # [64 * 27 * 27, 12]\n",
    "        output = unfolded_output.view(batch_size, out_height, out_width, out_channels)  # [64, 27, 27, 12]\n",
    "        output = output.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        output += self.b.view(1, out_channels, 1, 1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 12, 12])"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = torch.randn((64, 32, 13, 13), device=\"cuda:0\", requires_grad=True)\n",
    "conv_layer = Conv2d(out_channels=16, in_channels=32, kernel_size=(2, 2), stride=1, padding=0)\n",
    "conv_layer(input1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar output: 0.1838589906692505\n",
      "Gradient of input1: tensor([[[[-1.5735e-05, -4.5095e-05, -4.5095e-05,  ..., -4.5095e-05,\n",
      "           -4.5095e-05, -2.9360e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          ...,\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-2.3074e-07, -9.2455e-07, -9.2455e-07,  ..., -9.2455e-07,\n",
      "           -9.2455e-07, -6.9381e-07]],\n",
      "\n",
      "         [[ 8.5267e-06,  2.7429e-05,  2.7429e-05,  ...,  2.7429e-05,\n",
      "            2.7429e-05,  1.8902e-05],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          ...,\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 2.7830e-06, -9.5927e-06, -9.5927e-06,  ..., -9.5927e-06,\n",
      "           -9.5927e-06, -1.2376e-05]],\n",
      "\n",
      "         [[-1.4225e-06,  4.4118e-05,  4.4118e-05,  ...,  4.4118e-05,\n",
      "            4.4118e-05,  4.5541e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          ...,\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-9.3802e-06, -3.6084e-05, -3.6084e-05,  ..., -3.6084e-05,\n",
      "           -3.6084e-05, -2.6704e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1382e-05, -4.3267e-05, -4.3267e-05,  ..., -4.3267e-05,\n",
      "           -4.3267e-05, -1.1885e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          ...,\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 3.5385e-05,  5.7984e-05,  5.7984e-05,  ...,  5.7984e-05,\n",
      "            5.7984e-05,  2.2599e-05]],\n",
      "\n",
      "         [[-2.0069e-05,  3.3328e-05,  3.3328e-05,  ...,  3.3328e-05,\n",
      "            3.3328e-05,  5.3398e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          ...,\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-2.6170e-05, -6.1819e-05, -6.1819e-05,  ..., -6.1819e-05,\n",
      "           -6.1819e-05, -3.5648e-05]],\n",
      "\n",
      "         [[-2.6693e-05, -3.1961e-05, -3.1961e-05,  ..., -3.1961e-05,\n",
      "           -3.1961e-05, -5.2681e-06],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          ...,\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [ 6.1499e-06,  2.6548e-05,  2.6548e-05,  ...,  2.6548e-05,\n",
      "            2.6548e-05,  2.0398e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.5735e-05, -4.5095e-05, -4.5095e-05,  ..., -4.5095e-05,\n",
      "           -4.5095e-05, -2.9360e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          ...,\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-2.3074e-07, -9.2455e-07, -9.2455e-07,  ..., -9.2455e-07,\n",
      "           -9.2455e-07, -6.9381e-07]],\n",
      "\n",
      "         [[ 8.5267e-06,  2.7429e-05,  2.7429e-05,  ...,  2.7429e-05,\n",
      "            2.7429e-05,  1.8902e-05],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          ...,\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 2.7830e-06, -9.5927e-06, -9.5927e-06,  ..., -9.5927e-06,\n",
      "           -9.5927e-06, -1.2376e-05]],\n",
      "\n",
      "         [[-1.4225e-06,  4.4118e-05,  4.4118e-05,  ...,  4.4118e-05,\n",
      "            4.4118e-05,  4.5541e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          ...,\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-9.3802e-06, -3.6084e-05, -3.6084e-05,  ..., -3.6084e-05,\n",
      "           -3.6084e-05, -2.6704e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1382e-05, -4.3267e-05, -4.3267e-05,  ..., -4.3267e-05,\n",
      "           -4.3267e-05, -1.1885e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          ...,\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 3.5385e-05,  5.7984e-05,  5.7984e-05,  ...,  5.7984e-05,\n",
      "            5.7984e-05,  2.2599e-05]],\n",
      "\n",
      "         [[-2.0069e-05,  3.3328e-05,  3.3328e-05,  ...,  3.3328e-05,\n",
      "            3.3328e-05,  5.3398e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          ...,\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-2.6170e-05, -6.1819e-05, -6.1819e-05,  ..., -6.1819e-05,\n",
      "           -6.1819e-05, -3.5648e-05]],\n",
      "\n",
      "         [[-2.6693e-05, -3.1961e-05, -3.1961e-05,  ..., -3.1961e-05,\n",
      "           -3.1961e-05, -5.2681e-06],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          ...,\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [ 6.1499e-06,  2.6548e-05,  2.6548e-05,  ...,  2.6548e-05,\n",
      "            2.6548e-05,  2.0398e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.5735e-05, -4.5095e-05, -4.5095e-05,  ..., -4.5095e-05,\n",
      "           -4.5095e-05, -2.9360e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          ...,\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-2.3074e-07, -9.2455e-07, -9.2455e-07,  ..., -9.2455e-07,\n",
      "           -9.2455e-07, -6.9381e-07]],\n",
      "\n",
      "         [[ 8.5267e-06,  2.7429e-05,  2.7429e-05,  ...,  2.7429e-05,\n",
      "            2.7429e-05,  1.8902e-05],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          ...,\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 2.7830e-06, -9.5927e-06, -9.5927e-06,  ..., -9.5927e-06,\n",
      "           -9.5927e-06, -1.2376e-05]],\n",
      "\n",
      "         [[-1.4225e-06,  4.4118e-05,  4.4118e-05,  ...,  4.4118e-05,\n",
      "            4.4118e-05,  4.5541e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          ...,\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-9.3802e-06, -3.6084e-05, -3.6084e-05,  ..., -3.6084e-05,\n",
      "           -3.6084e-05, -2.6704e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1382e-05, -4.3267e-05, -4.3267e-05,  ..., -4.3267e-05,\n",
      "           -4.3267e-05, -1.1885e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          ...,\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 3.5385e-05,  5.7984e-05,  5.7984e-05,  ...,  5.7984e-05,\n",
      "            5.7984e-05,  2.2599e-05]],\n",
      "\n",
      "         [[-2.0069e-05,  3.3328e-05,  3.3328e-05,  ...,  3.3328e-05,\n",
      "            3.3328e-05,  5.3398e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          ...,\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-2.6170e-05, -6.1819e-05, -6.1819e-05,  ..., -6.1819e-05,\n",
      "           -6.1819e-05, -3.5648e-05]],\n",
      "\n",
      "         [[-2.6693e-05, -3.1961e-05, -3.1961e-05,  ..., -3.1961e-05,\n",
      "           -3.1961e-05, -5.2681e-06],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          ...,\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [ 6.1499e-06,  2.6548e-05,  2.6548e-05,  ...,  2.6548e-05,\n",
      "            2.6548e-05,  2.0398e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.5735e-05, -4.5095e-05, -4.5095e-05,  ..., -4.5095e-05,\n",
      "           -4.5095e-05, -2.9360e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          ...,\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-2.3074e-07, -9.2455e-07, -9.2455e-07,  ..., -9.2455e-07,\n",
      "           -9.2455e-07, -6.9381e-07]],\n",
      "\n",
      "         [[ 8.5267e-06,  2.7429e-05,  2.7429e-05,  ...,  2.7429e-05,\n",
      "            2.7429e-05,  1.8902e-05],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          ...,\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 2.7830e-06, -9.5927e-06, -9.5927e-06,  ..., -9.5927e-06,\n",
      "           -9.5927e-06, -1.2376e-05]],\n",
      "\n",
      "         [[-1.4225e-06,  4.4118e-05,  4.4118e-05,  ...,  4.4118e-05,\n",
      "            4.4118e-05,  4.5541e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          ...,\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-9.3802e-06, -3.6084e-05, -3.6084e-05,  ..., -3.6084e-05,\n",
      "           -3.6084e-05, -2.6704e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1382e-05, -4.3267e-05, -4.3267e-05,  ..., -4.3267e-05,\n",
      "           -4.3267e-05, -1.1885e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          ...,\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 3.5385e-05,  5.7984e-05,  5.7984e-05,  ...,  5.7984e-05,\n",
      "            5.7984e-05,  2.2599e-05]],\n",
      "\n",
      "         [[-2.0069e-05,  3.3328e-05,  3.3328e-05,  ...,  3.3328e-05,\n",
      "            3.3328e-05,  5.3398e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          ...,\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-2.6170e-05, -6.1819e-05, -6.1819e-05,  ..., -6.1819e-05,\n",
      "           -6.1819e-05, -3.5648e-05]],\n",
      "\n",
      "         [[-2.6693e-05, -3.1961e-05, -3.1961e-05,  ..., -3.1961e-05,\n",
      "           -3.1961e-05, -5.2681e-06],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          ...,\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [ 6.1499e-06,  2.6548e-05,  2.6548e-05,  ...,  2.6548e-05,\n",
      "            2.6548e-05,  2.0398e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.5735e-05, -4.5095e-05, -4.5095e-05,  ..., -4.5095e-05,\n",
      "           -4.5095e-05, -2.9360e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          ...,\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-2.3074e-07, -9.2455e-07, -9.2455e-07,  ..., -9.2455e-07,\n",
      "           -9.2455e-07, -6.9381e-07]],\n",
      "\n",
      "         [[ 8.5267e-06,  2.7429e-05,  2.7429e-05,  ...,  2.7429e-05,\n",
      "            2.7429e-05,  1.8902e-05],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          ...,\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 2.7830e-06, -9.5927e-06, -9.5927e-06,  ..., -9.5927e-06,\n",
      "           -9.5927e-06, -1.2376e-05]],\n",
      "\n",
      "         [[-1.4225e-06,  4.4118e-05,  4.4118e-05,  ...,  4.4118e-05,\n",
      "            4.4118e-05,  4.5541e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          ...,\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-9.3802e-06, -3.6084e-05, -3.6084e-05,  ..., -3.6084e-05,\n",
      "           -3.6084e-05, -2.6704e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1382e-05, -4.3267e-05, -4.3267e-05,  ..., -4.3267e-05,\n",
      "           -4.3267e-05, -1.1885e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          ...,\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 3.5385e-05,  5.7984e-05,  5.7984e-05,  ...,  5.7984e-05,\n",
      "            5.7984e-05,  2.2599e-05]],\n",
      "\n",
      "         [[-2.0069e-05,  3.3328e-05,  3.3328e-05,  ...,  3.3328e-05,\n",
      "            3.3328e-05,  5.3398e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          ...,\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-2.6170e-05, -6.1819e-05, -6.1819e-05,  ..., -6.1819e-05,\n",
      "           -6.1819e-05, -3.5648e-05]],\n",
      "\n",
      "         [[-2.6693e-05, -3.1961e-05, -3.1961e-05,  ..., -3.1961e-05,\n",
      "           -3.1961e-05, -5.2681e-06],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          ...,\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [ 6.1499e-06,  2.6548e-05,  2.6548e-05,  ...,  2.6548e-05,\n",
      "            2.6548e-05,  2.0398e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.5735e-05, -4.5095e-05, -4.5095e-05,  ..., -4.5095e-05,\n",
      "           -4.5095e-05, -2.9360e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          ...,\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-1.5965e-05, -4.6020e-05, -4.6020e-05,  ..., -4.6020e-05,\n",
      "           -4.6020e-05, -3.0054e-05],\n",
      "          [-2.3074e-07, -9.2455e-07, -9.2455e-07,  ..., -9.2455e-07,\n",
      "           -9.2455e-07, -6.9381e-07]],\n",
      "\n",
      "         [[ 8.5267e-06,  2.7429e-05,  2.7429e-05,  ...,  2.7429e-05,\n",
      "            2.7429e-05,  1.8902e-05],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          ...,\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 1.1310e-05,  1.7836e-05,  1.7836e-05,  ...,  1.7836e-05,\n",
      "            1.7836e-05,  6.5263e-06],\n",
      "          [ 2.7830e-06, -9.5927e-06, -9.5927e-06,  ..., -9.5927e-06,\n",
      "           -9.5927e-06, -1.2376e-05]],\n",
      "\n",
      "         [[-1.4225e-06,  4.4118e-05,  4.4118e-05,  ...,  4.4118e-05,\n",
      "            4.4118e-05,  4.5541e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          ...,\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-1.0803e-05,  8.0341e-06,  8.0341e-06,  ...,  8.0341e-06,\n",
      "            8.0341e-06,  1.8837e-05],\n",
      "          [-9.3802e-06, -3.6084e-05, -3.6084e-05,  ..., -3.6084e-05,\n",
      "           -3.6084e-05, -2.6704e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1382e-05, -4.3267e-05, -4.3267e-05,  ..., -4.3267e-05,\n",
      "           -4.3267e-05, -1.1885e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          ...,\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 4.0037e-06,  1.4717e-05,  1.4717e-05,  ...,  1.4717e-05,\n",
      "            1.4717e-05,  1.0714e-05],\n",
      "          [ 3.5385e-05,  5.7984e-05,  5.7984e-05,  ...,  5.7984e-05,\n",
      "            5.7984e-05,  2.2599e-05]],\n",
      "\n",
      "         [[-2.0069e-05,  3.3328e-05,  3.3328e-05,  ...,  3.3328e-05,\n",
      "            3.3328e-05,  5.3398e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          ...,\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-4.6240e-05, -2.8490e-05, -2.8490e-05,  ..., -2.8490e-05,\n",
      "           -2.8490e-05,  1.7749e-05],\n",
      "          [-2.6170e-05, -6.1819e-05, -6.1819e-05,  ..., -6.1819e-05,\n",
      "           -6.1819e-05, -3.5648e-05]],\n",
      "\n",
      "         [[-2.6693e-05, -3.1961e-05, -3.1961e-05,  ..., -3.1961e-05,\n",
      "           -3.1961e-05, -5.2681e-06],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          ...,\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [-2.0543e-05, -5.4134e-06, -5.4134e-06,  ..., -5.4134e-06,\n",
      "           -5.4134e-06,  1.5130e-05],\n",
      "          [ 6.1499e-06,  2.6548e-05,  2.6548e-05,  ...,  2.6548e-05,\n",
      "            2.6548e-05,  2.0398e-05]]]], device='cuda:0')\n",
      "Gradient of w1: None\n"
     ]
    }
   ],
   "source": [
    "output1 = conv_layer(input1)\n",
    "scalar_output = output1.mean()\n",
    "\n",
    "# Perform backpropagation\n",
    "scalar_output.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param in conv_layer.parameters():\n",
    "        param -= 0.1 * param.grad\n",
    "\n",
    "print(f'Scalar output: {scalar_output.item()}')\n",
    "print(f'Gradient of input1: {input1.grad}')\n",
    "print(f'Gradient of w1: {w1.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 6, 6])"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxPool(input, w, processor=\"cuda:0\"):\n",
    "    batch_size, in_channels, in_height, in_width = input.shape\n",
    "    kernel_height, kernel_width = w\n",
    "\n",
    "    out_height = in_height // kernel_height\n",
    "    out_width = in_width // kernel_width\n",
    "\n",
    "    output = torch.zeros(batch_size, in_channels, out_height, out_width, device=processor) # [64, 27, 27, 4]\n",
    "\n",
    "    for i in range(out_height):\n",
    "        for j in range(out_width):\n",
    "            start_i = i * kernel_height\n",
    "            end_i = start_i + kernel_height\n",
    "            start_j = j * kernel_width\n",
    "            end_j = start_j + kernel_width\n",
    "\n",
    "            window = input[:, :, start_i:end_i, start_j:end_j]\n",
    "            max_values_height, _ = window.max(dim=2, keepdim=True)  # Compute max along height dimension\n",
    "            max_values, _ = max_values_height.max(dim=3, keepdim=True)  # Compute max along width dimension\n",
    "            output[:, :, i, j] = max_values.squeeze() # do not first dim as 1\n",
    "            \n",
    "    return output\n",
    "\n",
    "input2 = torch.randn((64, 16, 12, 12), device=\"cuda:0\", requires_grad=True)\n",
    "output2 = maxPool(input2, (2, 2))\n",
    "output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d:\n",
    "    def __init__(self, w_size, processor=\"cuda:0\"):\n",
    "        self.w_size = w_size\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, input):\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        kernel_height, kernel_width = self.w_size\n",
    "\n",
    "        out_height = in_height // kernel_height\n",
    "        out_width = in_width // kernel_width\n",
    "\n",
    "        output = torch.zeros(batch_size, in_channels, out_height, out_width, device=self.processor) # [64, 27, 27, 4]\n",
    "\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                start_i = i * kernel_height\n",
    "                end_i = start_i + kernel_height\n",
    "                start_j = j * kernel_width\n",
    "                end_j = start_j + kernel_width\n",
    "\n",
    "                window = input[:, :, start_i:end_i, start_j:end_j]\n",
    "                max_values_height, _ = window.max(dim=2, keepdim=True)  # Compute max along height dimension\n",
    "                max_values, _ = max_values_height.max(dim=3, keepdim=True)  # Compute max along width dimension\n",
    "                output[:, :, i, j] = max_values.squeeze() # do not first dim as 1\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, parameters, lr, name):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.name = name\n",
    "\n",
    "        self.momentum = 0.9\n",
    "        self.velocities = [torch.zeros_like(p) for p in parameters]\n",
    "\n",
    "        self.decay_rate = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.sq_grads = [torch.zeros_like(p) for p in parameters]\n",
    "\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.m_t = [torch.zeros_like(p) for p in parameters]\n",
    "        self.v_t = [torch.zeros_like(p) for p in parameters]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        if self.name == \"stochastic\":\n",
    "            self.stochastic_step()\n",
    "        elif self.name == \"momentum\":\n",
    "            self.momentum_step()\n",
    "        elif self.name == \"RMSprop\":\n",
    "            self.RMSprop_step()\n",
    "        elif self.name == \"Adam\":\n",
    "            self.Adam_step()\n",
    "        else:\n",
    "            print(\"no valid optimizer with such name\")\n",
    "\n",
    "    def stochastic_step(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters:\n",
    "                if param is not None:\n",
    "                    param -= param.grad * self.lr\n",
    "                    # param.grad.zero_()\n",
    "\n",
    "    def momentum_step(self):\n",
    "        with torch.no_grad():\n",
    "            for param, velocity in zip(self.parameters, self.velocities):\n",
    "                if param is not None:\n",
    "                    # view momentum as the mass that scales the last velocity a bit but still gives more direction\n",
    "                    # the rest is just the stochatic gradient descent\n",
    "                    velocity.mul_(self.momentum).add_(param.grad, alpha=self.lr)\n",
    "                    param.sub_(velocity)\n",
    "                    # param.grad.zero_()\n",
    "\n",
    "                    # another way\n",
    "                    # higher the momentum closer it pays attention to the upcoming points\n",
    "                    # lower the momentum closer it pays attention to the previous points\n",
    "                    # velocity.mul_(self.momentum).add_(param.grad, alpha=(1 - self.momentum))\n",
    "                    # param.sub_(velocity, alpha=self.lr)\n",
    "                    # param.grad.zero_()\n",
    "    \n",
    "    def RMSprop_step(self):\n",
    "        with torch.no_grad():\n",
    "            for param, sq_grad in zip(self.parameters, self.sq_grads):\n",
    "                if param.grad is not None:\n",
    "                    sq_grad.mul_(self.decay_rate).addcmul_(param.grad, param.grad, value=1 - self.decay_rate)\n",
    "                    sqrt = sq_grad.sqrt().add(self.epsilon)\n",
    "                    adjusted_grad = param.grad / sqrt\n",
    "                    param.sub_(adjusted_grad, alpha=self.lr)\n",
    "\n",
    "    def Adam_step(self):\n",
    "        with torch.no_grad():\n",
    "            self.t += 1\n",
    "            for param, m, v in zip(self.parameters, self.m_t, self.v_t):\n",
    "                if param is not None:\n",
    "                    m.mul_(self.beta1).add_(param.grad, alpha=1 - self.beta1)\n",
    "                    v.mul_(self.beta2).addcmul_(param.grad, param.grad, value=1 - self.beta2)\n",
    "                    m_hat = m / (1 - self.beta1 ** self.t)\n",
    "                    v_hat = v / (1 - self.beta2 ** self.t)\n",
    "                    param.sub_(m_hat / (v_hat.sqrt().add(self.epsilon)), alpha=self.lr)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer: \n",
    "    # nin is the number of input (prev layer)\n",
    "    # nout is the number of output (next layer)\n",
    "    def __init__(self, nin, nout, processor):\n",
    "        self.w = torch.randn((nin, nout), requires_grad=True, device=processor)\n",
    "        self.b = torch.randn(1, requires_grad=True, device=processor)\n",
    "    \n",
    "    # x is the input in (batch, input)\n",
    "    def __call__(self, x):\n",
    "        eq = torch.matmul(x, self.w) + self.b\n",
    "        out = torch.tanh(eq)\n",
    "        return out # returns (batch, output # of neurons for next layer)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts, processor=\"cuda:0\"):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1], processor) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN():\n",
    "    def __init__(self, *layers, lr, optimizer=\"stochastic\"):\n",
    "        self.layers = layers\n",
    "        self.optimizer = Optimizer(self.parameters(), lr, optimizer)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grads(self):\n",
    "        self.optimizer.zero_grads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cnn, loss_fn):\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(\"cuda:0\"), y.to(\"cuda:0\")\n",
    "        \n",
    "        output = cnn(x)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        cnn.zero_grads()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        cnn.update_parameters()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Loss: {loss:>7f}\\t [{((batch + 1) * 64):>5d}/{937 * 64}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(cnn, loss_fn):\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dataloader:\n",
    "            x, y = x.to(\"cuda:0\"), y.to(\"cuda:0\")\n",
    "\n",
    "            output = cnn(x)\n",
    "\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "            tot += y.size(0)\n",
    "\n",
    "    accuracy = correct / tot * 100\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(cnn):\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_x, test_y in test_dataloader:\n",
    "            test_x, test_y = test_x.to(\"cuda:0\"), test_y.to(\"cuda:0\")\n",
    "\n",
    "            output = cnn(test_x)\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(test_y.view_as(pred)).sum().item()\n",
    "            tot += test_y.size(0)\n",
    "\n",
    "    accuracy = correct / tot * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(optimizer, learning_rates, layers, num_trials):\n",
    "    best_accuracy = None\n",
    "    best_lr = None\n",
    "    best_epochs = None\n",
    "    best_neurons = None\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        lr = random.choice(learning_rates)\n",
    "        layer = random.choice(layers)\n",
    "                    \n",
    "        cnn = CNN(layer, lr=lr, optimizer=optimizer)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        print(\"------------------------------------------------------------------------------------\")\n",
    "        print(f\"trial: {trial} optimizer: {optimizer}\\t lr: {lr}\\t layer: {layer}\")\n",
    "\n",
    "        new_accuracy = 0.0\n",
    "        prev_accuracy = -1\n",
    "\n",
    "        for i in range(30):\n",
    "            if prev_accuracy == new_accuracy:\n",
    "                print(f\"\\tNo accuracy change {prev_accuracy:.2f} == {new_accuracy:.2f}\")\n",
    "                break\n",
    "            prev_accuracy = new_accuracy\n",
    "            train(cnn, loss_fn)\n",
    "            new_loss, new_accuracy = validate(cnn, loss_fn)\n",
    "            print(f\"\\tEpoch {i + 1} Accuracy: {new_accuracy:.2f}\")\n",
    "\n",
    "            if new_accuracy > best_accuracy:\n",
    "                best_accuracy = new_accuracy\n",
    "                best_lr = lr\n",
    "                best_epochs = i + 1\n",
    "        \n",
    "        print(f\"loss: {new_loss}\\t accuracy: {new_accuracy:.2f}\\n\")\n",
    "\n",
    "    print(f\"accuracy: {best_accuracy:.2f}, optimizer: {optimizer}, lr: {best_lr}, epoch {best_epochs}, #neurons {best_neurons}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.0015, 0.002, 0.0025, 0.003, 0.0035, 0.004, 0.005]\n",
    "layers = [[Conv2d(4, 1, (2, 2)), MaxPool2d((2, 2)), MLP(676, [64, 32, 10])], \n",
    "          [Conv2d(12, 1, (2, 2)), Conv2d(4, 12, (2, 2)), MaxPool2d((2, 2)), MLP(676, [64, 32, 10])],\n",
    "          [Conv2d(4, 1, (2, 2)), Conv2d(12, 4, (2, 2)), MaxPool2d((3, 3)), MLP(972, [64, 32, 10])],\n",
    "          [Conv2d(8, 1, (1, 1)), Conv2d(4, 8, (2, 2)), MaxPool2d((2, 2)), MLP(676, [64, 32, 10])],\n",
    "          [Conv2d(4, 1, (1, 1)), MaxPool2d((2, 2)), MLP(784, [64, 32, 10])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers = [\"stochastic\", \"momentum\", \"RMSprop\", \"Adam\"]\n",
    "# learning_rates = [0.00001, 0.0001, 0.001, 0.002 0.01, 0.1, 0.14, 0.18, 0.2, 0.25, 0.3, 0.35]\n",
    "# neurons = [[256, 10], [512, 10], [48, 48, 10], \n",
    "#            [256, 48, 10], [48, 256, 10], [256, 256, 10], [512, 256, 10], [256, 512, 10], [512, 512, 10],\n",
    "#             [48, 48, 48, 10], [256, 256, 256, 10], [256, 512, 48, 10]]\n",
    "\n",
    "# tune_hyperparameters(optimizers=optimizers, learning_rates=learning_rates, neurons=neurons, epoch=30, num_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_output(mlp):\n",
    "    tot = 0\n",
    "    wrong = 0\n",
    "    right = 0\n",
    "\n",
    "    for test_x, test_y in test_dataloader:\n",
    "        test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "\n",
    "        test_x_flattened = test_x.view(test_x.size(0), -1)\n",
    "\n",
    "        output = mlp(test_x_flattened)\n",
    "        \n",
    "        for pred, act, img in zip(output, test_y, test_x):\n",
    "            pred_index = torch.argmax(pred).item()\n",
    "            act_index = act.item()\n",
    "\n",
    "            print(f\"Predicted: {labels_map[pred_index]}, Number: {torch.max(output).item()}\")\n",
    "            print(f\"Target: {labels_map[act_index]}, Number: {act.item()}\")\n",
    "            img = img.cpu()\n",
    "            plt.imshow(img[0], cmap=\"grey\")\n",
    "            plt.show()\n",
    "\n",
    "            if pred_index == act_index:\n",
    "                print(\"WRONG\")\n",
    "                right += 1\n",
    "            else: \n",
    "                print(\"WRONG\")\n",
    "                wrong += 1\n",
    "\n",
    "            tot += 1\n",
    "\n",
    "    right_ratio = right / tot * 100\n",
    "    print(f\"Test Accuracy: {right_ratio}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(Conv2d(16, 1, (2, 2)), \n",
    "          Conv2d(8, 16, (2, 2)),\n",
    "          MaxPool2d((4, 4)),\n",
    "        #   Conv2d(32, 32, (2, 2)), \n",
    "        #   MaxPool2d((2, 2)),\n",
    "          MLP(288, [32, 32, 10]), lr=0.002, optimizer=\"Adam\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 2, 2])\n",
      "torch.Size([16])\n",
      "torch.Size([8, 16, 2, 2])\n",
      "torch.Size([8])\n",
      "torch.Size([288, 32])\n",
      "torch.Size([1])\n",
      "torch.Size([32, 32])\n",
      "torch.Size([1])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for param in cnn.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-----------------------------------------\n",
      "Loss: 2.307483\t [   64/59968]\n",
      "Loss: 2.271355\t [ 6464/59968]\n",
      "Loss: 2.258699\t [12864/59968]\n",
      "Loss: 2.263895\t [19264/59968]\n",
      "Loss: 2.218279\t [25664/59968]\n",
      "Loss: 2.180628\t [32064/59968]\n",
      "Loss: 2.183852\t [38464/59968]\n",
      "Loss: 2.185267\t [44864/59968]\n",
      "Loss: 2.136875 Accuracy: 46.733333\n",
      "\n",
      "Epoch 2\n",
      "-----------------------------------------\n",
      "Loss: 2.173266\t [   64/59968]\n",
      "Loss: 2.138861\t [ 6464/59968]\n",
      "Loss: 2.159151\t [12864/59968]\n",
      "Loss: 2.150241\t [19264/59968]\n",
      "Loss: 2.154199\t [25664/59968]\n",
      "Loss: 2.129819\t [32064/59968]\n",
      "Loss: 2.104487\t [38464/59968]\n",
      "Loss: 2.120555\t [44864/59968]\n",
      "Loss: 2.110158 Accuracy: 57.100000\n",
      "\n",
      "Epoch 3\n",
      "-----------------------------------------\n",
      "Loss: 2.147846\t [   64/59968]\n",
      "Loss: 2.118898\t [ 6464/59968]\n",
      "Loss: 2.137908\t [12864/59968]\n",
      "Loss: 2.099808\t [19264/59968]\n",
      "Loss: 2.131430\t [25664/59968]\n",
      "Loss: 2.116397\t [32064/59968]\n",
      "Loss: 2.078150\t [38464/59968]\n",
      "Loss: 2.094715\t [44864/59968]\n",
      "Loss: 2.059473 Accuracy: 64.033333\n",
      "\n",
      "Epoch 4\n",
      "-----------------------------------------\n",
      "Loss: 2.117486\t [   64/59968]\n",
      "Loss: 2.105868\t [ 6464/59968]\n",
      "Loss: 2.095228\t [12864/59968]\n",
      "Loss: 2.096593\t [19264/59968]\n",
      "Loss: 2.079738\t [25664/59968]\n",
      "Loss: 2.086930\t [32064/59968]\n",
      "Loss: 2.059981\t [38464/59968]\n",
      "Loss: 2.077802\t [44864/59968]\n",
      "Loss: 2.047502 Accuracy: 67.866667\n",
      "\n",
      "Epoch 5\n",
      "-----------------------------------------\n",
      "Loss: 2.109764\t [   64/59968]\n",
      "Loss: 2.085298\t [ 6464/59968]\n",
      "Loss: 2.116111\t [12864/59968]\n",
      "Loss: 2.091028\t [19264/59968]\n",
      "Loss: 2.082839\t [25664/59968]\n",
      "Loss: 2.071571\t [32064/59968]\n",
      "Loss: 2.070494\t [38464/59968]\n",
      "Loss: 2.050868\t [44864/59968]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[656], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m validate(cnn, loss_fn)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>7f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[648], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(cnn, loss_fn)\u001b[0m\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, y)\n\u001b[0;32m      9\u001b[0m cnn\u001b[38;5;241m.\u001b[39mzero_grads()\n\u001b[1;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m cnn\u001b[38;5;241m.\u001b[39mupdate_parameters()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Elene2004\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Elene2004\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Elene2004\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    print(f\"Epoch {i + 1}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    train(cnn, loss_fn)\n",
    "    loss, accuracy = validate(cnn, loss_fn)\n",
    "    print(f\"Loss: {loss:>7f} Accuracy: {accuracy:>5f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 79.61%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
