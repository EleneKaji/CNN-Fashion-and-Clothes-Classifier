{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4785,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader # loads data either in chunks or full\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets # open datasets\n",
    "from torchvision.transforms import ToTensor # transfor data to tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 4786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choosing gpu for training\n",
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4787,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download training and test data from open datasets.\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4788,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "train_dataloader = DataLoader(train_data, batch_size=64) # goes over 938 batches of 64\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Sandal, Number: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdm0lEQVR4nO3df2xV9f3H8delyAW0vbNCe2/5URuH01nECAxsQAubDc0kIrqhJg6yxOgEHCvGyUhmtz+ocZG4BGXOLAgTNv5RZwYRu2GLWlkQcSAjroYyukDTUfHeUqCV9vP9g3C/1iLwOd57373t85HchN57Xpx3D4e+ONx7PzfknHMCAMDAEOsBAACDFyUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM0OtB/iynp4eHTlyRLm5uQqFQtbjAAA8OefU3t6uoqIiDRly4WudfldCR44c0bhx46zHAAB8Tc3NzRo7duwFt+l3/x2Xm5trPQIAIAUu5ed52kro+eefV0lJiYYPH67Jkyfr7bffvqQc/wUHAAPDpfw8T0sJbd68WcuWLdPKlSu1Z88ezZw5U5WVlTp8+HA6dgcAyFKhdKyiPW3aNN18881au3Zt8r7rr79e8+bNU01NzQWziURCkUgk1SMBADIsHo8rLy/vgtuk/Eqoq6tLu3fvVkVFRa/7Kyoq1NDQ0Gf7zs5OJRKJXjcAwOCQ8hI6duyYuru7VVhY2Ov+wsJCtbS09Nm+pqZGkUgkeeOVcQAweKTthQlffkLKOXfeJ6lWrFiheDyevDU3N6drJABAP5Py9wmNGjVKOTk5fa56Wltb+1wdSVI4HFY4HE71GACALJDyK6Fhw4Zp8uTJqq2t7XV/bW2tysrKUr07AEAWS8uKCVVVVXrggQc0ZcoU3XLLLfr973+vw4cP6+GHH07H7gAAWSotJbRgwQK1tbXp17/+tY4eParS0lJt3bpVxcXF6dgdACBLpeV9Ql8H7xMCgIHB5H1CAABcKkoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGbSsoo2kK3O9+m/FzNkiP+/5Xp6erwzmVxrOMhxCKKfrZ+cEkE+N62hocE7861vfcs7I0n//ve/vTPp/HPiSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYCbk+tkytolEQpFIxHoMDFKZWkW7u7vbO4PgysvLA+UmTpzonZkwYYJ35sYbb/TOBF3pvKKiwjvT2dkZaF/xeFx5eXkX3IYrIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGaGWg+AwSXIoouZXGM3yL7682KkP/rRjwLldu7c6Z2ZOXOmd+bRRx/1zhw5csQ7E2SBUElqbGz0znzwwQfemWXLlnlnPvzwQ+9Mf8SVEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADMsYAoYuO6667wzQ4f6/3UtLy/3zkjSlClTvDNXXnmld+all17yzuzYscM7E2RRUUmaPHmyd2bq1Knema6uLu/MN7/5Te+MJH3yySeBcunClRAAwAwlBAAwk/ISqq6uVigU6nWLRqOp3g0AYABIy3NCN9xwg/72t78lv87JyUnHbgAAWS4tJTR06FCufgAAF5WW54QaGxtVVFSkkpIS3XvvvTp48OBXbtvZ2alEItHrBgAYHFJeQtOmTdOGDRu0bds2vfjii2ppaVFZWZna2trOu31NTY0ikUjyNm7cuFSPBADop1JeQpWVlbr77rs1ceJEfe9739OWLVskSevXrz/v9itWrFA8Hk/empubUz0SAKCfSvubVS+//HJNnDhRjY2N5308HA4rHA6newwAQD+U9vcJdXZ26sCBA4rFYuneFQAgy6S8hB577DHV19erqalJ//jHP3TPPfcokUho4cKFqd4VACDLpfy/4/773//qvvvu07FjxzR69GhNnz5dO3fuVHFxcap3BQDIciHnnLMe4osSiYQikYj1GBikRo4c6Z0pKyvzzrS0tHhngrx9IeirTX/2s595Z44cOeKdefTRR70zBQUF3pnW1lbvjCTl5uZ6Z+6//37vzPDhw70zp0+f9s5I0gsvvBAoF0Q8HldeXt4Ft2HtOACAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGbS/qF2wBfl5OR4Z3p6erwzQdflveKKK7wzQRaSLC0t9c6Ul5d7Zx566CHvjCTNmTPHO7Nt27ZA+/IVdDHSIIIslvrpp596Z8aMGeOd+fGPf+ydkaR3333XO/PRRx8F2tel4EoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGVbSRUZlcETuIU6dOeWeGDPH/t9zs2bO9My+//LJ35uGHH/bO4P9dddVV3pm8vDzvzPvvv++d6ezs9M5IUjgc9s74Hoeenh4dP378krblSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAICZkMvk6pCXIJFIKBKJWI8BDAgjRowIlDt9+rR3JlM/SkKhkHcm6Gx33nmndybIgrYHDx70zsTjce+MJBUVFXlnfM+H7u5u/fOf/1Q8Hr/ogq5cCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADAz1HoAINvl5OR4Z3p6erwzQRbGDCrIvrq7u9Mwia3Ro0d7Z06cOOGdCbIoa5DzTpKuuOIK78yZM2e8tvf5frgSAgCYoYQAAGa8S2jHjh2aO3euioqKFAqF9Nprr/V63Dmn6upqFRUVacSIESovL9f+/ftTNS8AYADxLqGOjg5NmjRJa9asOe/jTz/9tFavXq01a9Zo165dikajuv3229Xe3v61hwUADCzeL0yorKxUZWXleR9zzunZZ5/VypUrNX/+fEnS+vXrVVhYqE2bNumhhx76etMCAAaUlD4n1NTUpJaWFlVUVCTvC4fDuu2229TQ0HDeTGdnpxKJRK8bAGBwSGkJtbS0SJIKCwt73V9YWJh87MtqamoUiUSSt3HjxqVyJABAP5aWV8d9+TXizrmvfN34ihUrFI/Hk7fm5uZ0jAQA6IdS+mbVaDQq6ewVUSwWS97f2tra5+ronHA4rHA4nMoxAABZIqVXQiUlJYpGo6qtrU3e19XVpfr6epWVlaVyVwCAAcD7SujEiRP65JNPkl83NTXpww8/VH5+vsaPH69ly5Zp1apVmjBhgiZMmKBVq1Zp5MiRuv/++1M6OAAg+3mX0Pvvv69Zs2Ylv66qqpIkLVy4UC+99JIef/xxnTp1So888oiOHz+uadOm6c0331Rubm7qpgYADAgh55yzHuKLEomEIpGI9RjAoBZkccwgC5gGWbgzkz+yfvrTn3pn7rnnHu/MX//6V+/Mpk2bvDOSAr0C2fetM93d3Tpw4IDi8bjy8vIuuC1rxwEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzKT0k1WRnYKsZCxldjVjZFaQFbGDrLwdRJDZgjp27Jh3Zs+ePd6ZKVOmeGdeeOEF74wkXXPNNd6ZhoYGr+17enoueVuuhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJgJuX62CmUikVAkErEeA8AActNNNwXKnTlzxjtz4MAB78wdd9zhnRk+fLh3RpKuuOIK78wf//hHr+2dc/r8888Vj8eVl5d3wW25EgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGBmqPUAAPqfnJwc70x3d3caJunr5z//uXcmPz8/0L7Wrl3rnXnggQe8M21tbd6ZrVu3emckqbi42DvT1dUVaF+XgishAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZljAFEAfQRYjvfrqq70z1dXV3pkgi6v+73//885I0j333OOdaWxs9M4MHer/o7ioqMg7I0mff/55oFy6cCUEADBDCQEAzHiX0I4dOzR37lwVFRUpFArptdde6/X4okWLFAqFet2mT5+eqnkBAAOIdwl1dHRo0qRJWrNmzVduM2fOHB09ejR5C/rhSwCAgc372bDKykpVVlZecJtwOKxoNBp4KADA4JCW54Tq6upUUFCga6+9Vg8++KBaW1u/ctvOzk4lEoleNwDA4JDyEqqsrNTGjRu1fft2PfPMM9q1a5dmz56tzs7O825fU1OjSCSSvI0bNy7VIwEA+qmUv09owYIFyV+XlpZqypQpKi4u1pYtWzR//vw+269YsUJVVVXJrxOJBEUEAINE2t+sGovFVFxc/JVv4AqHwwqHw+keAwDQD6X9fUJtbW1qbm5WLBZL964AAFnG+0roxIkT+uSTT5JfNzU16cMPP1R+fr7y8/NVXV2tu+++W7FYTIcOHdIvfvELjRo1SnfddVdKBwcAZD/vEnr//fc1a9as5Nfnns9ZuHCh1q5dq3379mnDhg367LPPFIvFNGvWLG3evFm5ubmpmxoAMCCEnHPOeogvSiQSikQi1mOkXJBFF4MsIonMC/JnGwqFvDPDhg3zzpw8edI7I0nXXXedd+Y3v/mNdybIYp9BXrj0wx/+0DsjSZn68XjTTTd5Z4I+xfHee+95Zz777LNA+4rH48rLy7vgNqwdBwAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwk/ZPVsVZmVoRO8jqzEH1swXYzQT5sw2y8naQFbHHjBnjnZGk5cuXe2e2b9/unZk+fbp35gc/+IF3pr8L8ncpyDkkBV9ZPV24EgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGBUwHGBYV/XqCLAAb5JhnakHb6urqQLkjR454ZyZNmuSdWbBggXdmIApyPowaNSrQvrq6ugLl0oUrIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGZYwDRDMrUw5je+8Q3vTGFhoXdGkmKxmHemrq4u0L4ypT8vAPurX/3KO3PmzJlA+7rxxhu9M3fddVegfWXC0KGZ+1EX5JgHmS/oAqb9DVdCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzLCAaYZkamHMb3/7296ZcePGBdpXIpHwzowcOdI7c/LkSe9MfzdmzBjvTFlZmXdm+PDh3hlJmjlzZqBcfxX0719PT0+KJzm/IPONHz8+DZNkHldCAAAzlBAAwIxXCdXU1Gjq1KnKzc1VQUGB5s2bp48//rjXNs45VVdXq6ioSCNGjFB5ebn279+f0qEBAAODVwnV19dr8eLF2rlzp2pra3XmzBlVVFSoo6Mjuc3TTz+t1atXa82aNdq1a5ei0ahuv/12tbe3p3x4AEB283phwhtvvNHr63Xr1qmgoEC7d+/WrbfeKuecnn32Wa1cuVLz58+XJK1fv16FhYXatGmTHnroodRNDgDIel/rOaF4PC5Jys/PlyQ1NTWppaVFFRUVyW3C4bBuu+02NTQ0nPf36OzsVCKR6HUDAAwOgUvIOaeqqirNmDFDpaWlkqSWlhZJUmFhYa9tCwsLk499WU1NjSKRSPIW9OXCAIDsE7iElixZor179+pPf/pTn8dCoVCvr51zfe47Z8WKFYrH48lbc3Nz0JEAAFkm0JtVly5dqtdff107duzQ2LFjk/dHo1FJZ6+IYrFY8v7W1tY+V0fnhMNhhcPhIGMAALKc15WQc05LlizRK6+8ou3bt6ukpKTX4yUlJYpGo6qtrU3e19XVpfr6+kDv9gYADGxeV0KLFy/Wpk2b9Je//EW5ubnJ53kikYhGjBihUCikZcuWadWqVZowYYImTJigVatWaeTIkbr//vvT8g0AALKXVwmtXbtWklReXt7r/nXr1mnRokWSpMcff1ynTp3SI488ouPHj2vatGl68803lZubm5KBAQADR8hlamXNS5RIJBSJRCT1fYHDhfSzb6MPn+/lnP7+PSG4LVu2eGfOvQrVx/e//33vjCR99NFHgXIDTab+3gb5s129erV3RlKvt9CkWzweV15e3gW3Ye04AIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAICZQJ+smikDaRXpTH0vQVb93bp1a6B9jRkzxjtTU1PjnTnfR8j3J7/85S+9M3PmzPHO/Pa3v/XOsBp2dhg61P9H8ZVXXpmGSTKPKyEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABm+u0CpjNmzPBa1K+rq8t7H4lEwjsjScePH/fOdHR0eGc6Ozu9M6dPn85IRpKuueYa78zy5cu9M3//+9+9M62trd4ZSaqoqPDOPProo96Z+vp678wTTzzhncHXk6mFh4cM8b8eCPr3tr/hSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAICZfruA6fjx4zVs2LBL3v7qq6/23sfo0aO9M5KUl5fnnfn888+9M59++ql3pqenxzvT3NzsnZGkjRs3emf27t3rnfnud7/rnSkrK/POSNKNN97onXn33Xe9M0EWcg2ySG84HPbOSMEWz0VwJ0+e9M68+eabaZgk87gSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYCbknHPWQ3xRIpFQJBKxHiPlrrrqKu/M2LFjvTP5+fkZ2Y8khUIh70xxcbF35vrrr/fO5Obmemck6Z133vHObNq0yTsTdNFYDExBFmD+4IMPAu0ryM+IoOLx+EUXfOZKCABghhICAJjxKqGamhpNnTpVubm5Kigo0Lx58/Txxx/32mbRokUKhUK9btOnT0/p0ACAgcGrhOrr67V48WLt3LlTtbW1OnPmjCoqKtTR0dFruzlz5ujo0aPJ29atW1M6NABgYPD6ZNU33nij19fr1q1TQUGBdu/erVtvvTV5fzgcVjQaTc2EAIAB62s9JxSPxyX1fbVFXV2dCgoKdO211+rBBx9Ua2vrV/4enZ2dSiQSvW4AgMEhcAk551RVVaUZM2aotLQ0eX9lZaU2btyo7du365lnntGuXbs0e/bsr/zM+pqaGkUikeRt3LhxQUcCAGQZr/+O+6IlS5Zo7969fd5XsWDBguSvS0tLNWXKFBUXF2vLli2aP39+n99nxYoVqqqqSn6dSCQoIgAYJAKV0NKlS/X6669rx44dF32jYywWU3FxsRobG8/7eDgcVjgcDjIGACDLeZWQc05Lly7Vq6++qrq6OpWUlFw009bWpubmZsViscBDAgAGJq/nhBYvXqyXX35ZmzZtUm5urlpaWtTS0qJTp05Jkk6cOKHHHntM7733ng4dOqS6ujrNnTtXo0aN0l133ZWWbwAAkL28roTWrl0rSSovL+91/7p167Ro0SLl5ORo37592rBhgz777DPFYjHNmjVLmzdvDryWFwBg4PL+77gLGTFihLZt2/a1BgIADB6BXx0HP21tbRnJAMg+hw4d8s4899xzqR/EAAuYAgDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMBNyF1saO8MSiYQikYj1GACArykejysvL++C23AlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAz/a6E+tlSdgCAgC7l53m/K6H29nbrEQAAKXApP8/73SraPT09OnLkiHJzcxUKhXo9lkgkNG7cODU3N190ZdaBjONwFsfhLI7DWRyHs/rDcXDOqb29XUVFRRoy5MLXOkMzNNMlGzJkiMaOHXvBbfLy8gb1SXYOx+EsjsNZHIezOA5nWR+HS/1Inn7333EAgMGDEgIAmMmqEgqHw3ryyScVDoetRzHFcTiL43AWx+EsjsNZ2XYc+t0LEwAAg0dWXQkBAAYWSggAYIYSAgCYoYQAAGayqoSef/55lZSUaPjw4Zo8ebLefvtt65Eyqrq6WqFQqNctGo1aj5V2O3bs0Ny5c1VUVKRQKKTXXnut1+POOVVXV6uoqEgjRoxQeXm59u/fbzNsGl3sOCxatKjP+TF9+nSbYdOkpqZGU6dOVW5urgoKCjRv3jx9/PHHvbYZDOfDpRyHbDkfsqaENm/erGXLlmnlypXas2ePZs6cqcrKSh0+fNh6tIy64YYbdPTo0eRt37591iOlXUdHhyZNmqQ1a9ac9/Gnn35aq1ev1po1a7Rr1y5Fo1HdfvvtA24dwosdB0maM2dOr/Nj69atGZww/err67V48WLt3LlTtbW1OnPmjCoqKtTR0ZHcZjCcD5dyHKQsOR9clvjOd77jHn744V73XXfdde6JJ54wmijznnzySTdp0iTrMUxJcq+++mry656eHheNRt1TTz2VvO/06dMuEom43/3udwYTZsaXj4Nzzi1cuNDdeeedJvNYaW1tdZJcfX29c27wng9fPg7OZc/5kBVXQl1dXdq9e7cqKip63V9RUaGGhgajqWw0NjaqqKhIJSUluvfee3Xw4EHrkUw1NTWppaWl17kRDod12223DbpzQ5Lq6upUUFCga6+9Vg8++KBaW1utR0qreDwuScrPz5c0eM+HLx+Hc7LhfMiKEjp27Ji6u7tVWFjY6/7CwkK1tLQYTZV506ZN04YNG7Rt2za9+OKLamlpUVlZmdra2qxHM3Puz3+wnxuSVFlZqY0bN2r79u165plntGvXLs2ePVudnZ3Wo6WFc05VVVWaMWOGSktLJQ3O8+F8x0HKnvOh362ifSFf/mgH51yf+wayysrK5K8nTpyoW265Rddcc43Wr1+vqqoqw8nsDfZzQ5IWLFiQ/HVpaammTJmi4uJibdmyRfPnzzecLD2WLFmivXv36p133unz2GA6H77qOGTL+ZAVV0KjRo1STk5On3/JtLa29vkXz2By+eWXa+LEiWpsbLQexcy5VwdybvQVi8VUXFw8IM+PpUuX6vXXX9dbb73V66NfBtv58FXH4Xz66/mQFSU0bNgwTZ48WbW1tb3ur62tVVlZmdFU9jo7O3XgwAHFYjHrUcyUlJQoGo32Oje6urpUX18/qM8NSWpra1Nzc/OAOj+cc1qyZIleeeUVbd++XSUlJb0eHyznw8WOw/n02/PB8EURXv785z+7yy67zP3hD39w//rXv9yyZcvc5Zdf7g4dOmQ9WsYsX77c1dXVuYMHD7qdO3e6O+64w+Xm5g74Y9De3u727Nnj9uzZ4yS51atXuz179rj//Oc/zjnnnnrqKReJRNwrr7zi9u3b5+677z4Xi8VcIpEwnjy1LnQc2tvb3fLly11DQ4Nrampyb731lrvlllvcmDFjBtRx+MlPfuIikYirq6tzR48eTd5OnjyZ3GYwnA8XOw7ZdD5kTQk559xzzz3niouL3bBhw9zNN9/c6+WIg8GCBQtcLBZzl112mSsqKnLz5893+/fvtx4r7d566y0nqc9t4cKFzrmzL8t98sknXTQadeFw2N16661u3759tkOnwYWOw8mTJ11FRYUbPXq0u+yyy9z48ePdwoUL3eHDh63HTqnzff+S3Lp165LbDIbz4WLHIZvOBz7KAQBgJiueEwIADEyUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDM/B/bdg4dbWOYogAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting and understanding images and labels with first batch\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "idx = torch.randint(0, 64, (1,)).item()\n",
    "image = images[idx].squeeze()\n",
    "label = labels[idx]\n",
    "print(f\"Name: {labels_map[label.item()]}, Number: {label.item()}\")\n",
    "plt.imshow(image, cmap=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4790,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer: \n",
    "    # nin is the number of input (prev layer)\n",
    "    # nout is the number of output (next layer)\n",
    "    def __init__(self, nin, nout, processor=\"cuda:0\"):\n",
    "        self.w = torch.randn((nin, nout), requires_grad=True, device=processor)\n",
    "        self.b = torch.randn(1, requires_grad=True, device=processor)\n",
    "    \n",
    "    # x is the input in (batch, input)\n",
    "    def __call__(self, x):\n",
    "        eq = torch.matmul(x, self.w) + self.b\n",
    "        out = torch.tanh(eq)\n",
    "        return out # returns (batch, output # of neurons for next layer)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts, lr, processor=\"cuda:0\"):\n",
    "        self.lr = lr\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                if param is not None:\n",
    "                    param -= param.grad * self.lr\n",
    "                    param.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4791,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(mlp):\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_x, test_y in test_dataloader:\n",
    "            test_x, test_y = test_x.to(\"cuda:0\"), test_y.to(\"cuda:0\")\n",
    "\n",
    "            test_x_flattened = test_x.view(test_x.size(0), -1)\n",
    "            output = mlp(test_x_flattened)\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(test_y.view_as(pred)).sum().item()\n",
    "            tot += test_y.size(0)\n",
    "\n",
    "    accuracy = correct / tot * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4792,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mlp, loss_fn):\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(\"cuda:0\"), y.to(\"cuda:0\")\n",
    "\n",
    "        x_flattened = x.view(x.size(0), -1)\n",
    "\n",
    "        output = mlp(x_flattened)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        mlp.update_parameters()\n",
    "\n",
    "        if batch % 200 == 0:\n",
    "            print(f\"Loss: {loss:>7f}\\t [{((batch + 1) * 64):>5d}/{937 * 64}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4793,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_output(mlp):\n",
    "    tot = 0\n",
    "    wrong = 0\n",
    "    right = 0\n",
    "\n",
    "    for test_x, test_y in test_dataloader:\n",
    "        test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "\n",
    "        test_x_flattened = test_x.view(test_x.size(0), -1)\n",
    "\n",
    "        output = mlp(test_x_flattened)\n",
    "        \n",
    "        for pred, act, img in zip(output, test_y, test_x):\n",
    "            pred_index = torch.argmax(pred).item()\n",
    "            act_index = act.item()\n",
    "\n",
    "            print(f\"Predicted: {labels_map[pred_index]}, Number: {torch.max(output).item()}\")\n",
    "            print(f\"Target: {labels_map[act_index]}, Number: {act.item()}\")\n",
    "            img = img.cpu()\n",
    "            plt.imshow(img[0], cmap=\"grey\")\n",
    "            plt.show()\n",
    "\n",
    "            if pred_index == act_index:\n",
    "                print(\"WRONG\")\n",
    "                right += 1\n",
    "            else: \n",
    "                print(\"WRONG\")\n",
    "                wrong += 1\n",
    "\n",
    "            tot += 1\n",
    "\n",
    "    right_ratio = right / tot * 100\n",
    "    print(f\"Test Accuracy: {right_ratio}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4794,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(784, [512, 512, 10], lr=0.3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4795,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-----------------------------------------\n",
      "Loss: 2.298992\t [   64/59968]\n",
      "Loss: 2.293360\t [12864/59968]\n",
      "Loss: 2.266915\t [25664/59968]\n",
      "Loss: 2.234633\t [38464/59968]\n",
      "Loss: 2.223827\t [51264/59968]\n",
      "Test Accuracy: 38.68%\n",
      "\n",
      "Epoch 2\n",
      "-----------------------------------------\n",
      "Loss: 2.199945\t [   64/59968]\n",
      "Loss: 2.174864\t [12864/59968]\n",
      "Loss: 2.174021\t [25664/59968]\n",
      "Loss: 2.147015\t [38464/59968]\n",
      "Loss: 2.140187\t [51264/59968]\n",
      "Test Accuracy: 56.46%\n",
      "\n",
      "Epoch 3\n",
      "-----------------------------------------\n",
      "Loss: 2.139389\t [   64/59968]\n",
      "Loss: 2.103467\t [12864/59968]\n",
      "Loss: 2.118001\t [25664/59968]\n",
      "Loss: 2.093300\t [38464/59968]\n",
      "Loss: 2.091694\t [51264/59968]\n",
      "Test Accuracy: 63.09%\n",
      "\n",
      "Epoch 4\n",
      "-----------------------------------------\n",
      "Loss: 2.131415\t [   64/59968]\n",
      "Loss: 2.071629\t [12864/59968]\n",
      "Loss: 2.095135\t [25664/59968]\n",
      "Loss: 2.070040\t [38464/59968]\n",
      "Loss: 2.083110\t [51264/59968]\n",
      "Test Accuracy: 67.14%\n",
      "\n",
      "Epoch 5\n",
      "-----------------------------------------\n",
      "Loss: 2.102574\t [   64/59968]\n",
      "Loss: 2.062670\t [12864/59968]\n",
      "Loss: 2.075408\t [25664/59968]\n",
      "Loss: 2.077241\t [38464/59968]\n",
      "Loss: 2.088291\t [51264/59968]\n",
      "Test Accuracy: 68.69%\n",
      "\n",
      "Epoch 6\n",
      "-----------------------------------------\n",
      "Loss: 2.082689\t [   64/59968]\n",
      "Loss: 2.057874\t [12864/59968]\n",
      "Loss: 2.072614\t [25664/59968]\n",
      "Loss: 2.089793\t [38464/59968]\n",
      "Loss: 2.088097\t [51264/59968]\n",
      "Test Accuracy: 70.62%\n",
      "\n",
      "Epoch 7\n",
      "-----------------------------------------\n",
      "Loss: 2.069531\t [   64/59968]\n",
      "Loss: 2.066829\t [12864/59968]\n",
      "Loss: 2.075095\t [25664/59968]\n",
      "Loss: 2.074260\t [38464/59968]\n",
      "Loss: 2.077294\t [51264/59968]\n",
      "Test Accuracy: 71.97%\n",
      "\n",
      "Epoch 8\n",
      "-----------------------------------------\n",
      "Loss: 2.056252\t [   64/59968]\n",
      "Loss: 2.042199\t [12864/59968]\n",
      "Loss: 2.069101\t [25664/59968]\n",
      "Loss: 2.066328\t [38464/59968]\n",
      "Loss: 2.071673\t [51264/59968]\n",
      "Test Accuracy: 71.88%\n",
      "\n",
      "Epoch 9\n",
      "-----------------------------------------\n",
      "Loss: 2.045598\t [   64/59968]\n",
      "Loss: 2.050179\t [12864/59968]\n",
      "Loss: 2.060525\t [25664/59968]\n",
      "Loss: 2.068279\t [38464/59968]\n",
      "Loss: 2.071680\t [51264/59968]\n",
      "Test Accuracy: 73.31%\n",
      "\n",
      "Epoch 10\n",
      "-----------------------------------------\n",
      "Loss: 2.044173\t [   64/59968]\n",
      "Loss: 2.042336\t [12864/59968]\n",
      "Loss: 2.045791\t [25664/59968]\n",
      "Loss: 2.064134\t [38464/59968]\n",
      "Loss: 2.057100\t [51264/59968]\n",
      "Test Accuracy: 73.84%\n",
      "\n",
      "Epoch 11\n",
      "-----------------------------------------\n",
      "Loss: 2.076153\t [   64/59968]\n",
      "Loss: 2.031882\t [12864/59968]\n",
      "Loss: 2.066559\t [25664/59968]\n",
      "Loss: 2.054918\t [38464/59968]\n",
      "Loss: 2.061165\t [51264/59968]\n",
      "Test Accuracy: 75.02%\n",
      "\n",
      "Epoch 12\n",
      "-----------------------------------------\n",
      "Loss: 2.058600\t [   64/59968]\n",
      "Loss: 2.025572\t [12864/59968]\n",
      "Loss: 2.051133\t [25664/59968]\n",
      "Loss: 2.060236\t [38464/59968]\n",
      "Loss: 2.053185\t [51264/59968]\n",
      "Test Accuracy: 75.13%\n",
      "\n",
      "Epoch 13\n",
      "-----------------------------------------\n",
      "Loss: 2.050909\t [   64/59968]\n",
      "Loss: 2.023914\t [12864/59968]\n",
      "Loss: 2.055177\t [25664/59968]\n",
      "Loss: 2.060427\t [38464/59968]\n",
      "Loss: 2.045518\t [51264/59968]\n",
      "Test Accuracy: 76.13%\n",
      "\n",
      "Epoch 14\n",
      "-----------------------------------------\n",
      "Loss: 2.048682\t [   64/59968]\n",
      "Loss: 2.020510\t [12864/59968]\n",
      "Loss: 2.052432\t [25664/59968]\n",
      "Loss: 2.049352\t [38464/59968]\n",
      "Loss: 2.055282\t [51264/59968]\n",
      "Test Accuracy: 76.39%\n",
      "\n",
      "Epoch 15\n",
      "-----------------------------------------\n",
      "Loss: 2.030094\t [   64/59968]\n",
      "Loss: 2.020525\t [12864/59968]\n",
      "Loss: 2.058653\t [25664/59968]\n",
      "Loss: 2.051236\t [38464/59968]\n",
      "Loss: 2.055297\t [51264/59968]\n",
      "Test Accuracy: 77.16%\n",
      "\n",
      "Epoch 16\n",
      "-----------------------------------------\n",
      "Loss: 2.033267\t [   64/59968]\n",
      "Loss: 2.020885\t [12864/59968]\n",
      "Loss: 2.048848\t [25664/59968]\n",
      "Loss: 2.044106\t [38464/59968]\n",
      "Loss: 2.066191\t [51264/59968]\n",
      "Test Accuracy: 76.39%\n",
      "\n",
      "Epoch 17\n",
      "-----------------------------------------\n",
      "Loss: 2.032533\t [   64/59968]\n",
      "Loss: 2.019317\t [12864/59968]\n",
      "Loss: 2.047221\t [25664/59968]\n",
      "Loss: 2.050004\t [38464/59968]\n",
      "Loss: 2.057464\t [51264/59968]\n",
      "Test Accuracy: 77.20%\n",
      "\n",
      "Epoch 18\n",
      "-----------------------------------------\n",
      "Loss: 2.041082\t [   64/59968]\n",
      "Loss: 2.019038\t [12864/59968]\n",
      "Loss: 2.046819\t [25664/59968]\n",
      "Loss: 2.042654\t [38464/59968]\n",
      "Loss: 2.059245\t [51264/59968]\n",
      "Test Accuracy: 77.14%\n",
      "\n",
      "Epoch 19\n",
      "-----------------------------------------\n",
      "Loss: 2.050850\t [   64/59968]\n",
      "Loss: 2.017525\t [12864/59968]\n",
      "Loss: 2.042466\t [25664/59968]\n",
      "Loss: 2.046814\t [38464/59968]\n",
      "Loss: 2.053559\t [51264/59968]\n",
      "Test Accuracy: 78.02%\n",
      "\n",
      "Epoch 20\n",
      "-----------------------------------------\n",
      "Loss: 2.027733\t [   64/59968]\n",
      "Loss: 2.004226\t [12864/59968]\n",
      "Loss: 2.034409\t [25664/59968]\n",
      "Loss: 2.048960\t [38464/59968]\n",
      "Loss: 2.056170\t [51264/59968]\n",
      "Test Accuracy: 78.09%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(f\"Epoch {i + 1}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    train(mlp, loss_fn)\n",
    "    test(mlp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
